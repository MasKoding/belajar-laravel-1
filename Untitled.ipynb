{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6bae31-7db9-483a-9bfe-3b0bc1bdf126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.10.15)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: jaxlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (4.25.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jax->mediapipe) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a39075-4228-4660-ab46-909762d01372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-13 13:26:45--  https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 34.101.5.27, 34.101.5.123, 142.251.221.155, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|34.101.5.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7819105 (7.5M) [application/octet-stream]\n",
      "Saving to: ‘hand_landmarker.task.2’\n",
      "\n",
      "hand_landmarker.tas 100%[===================>]   7.46M  1.08MB/s    in 6.8s    \n",
      "\n",
      "2024-09-13 13:26:52 (1.10 MB/s) - ‘hand_landmarker.task.2’ saved [7819105/7819105]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e7a4e2-f695-48a7-9196-5e50462cfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef2e286-7fcf-4fdc-8ad4-1d847e6c70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "          landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks_proto,\n",
    "          solutions.hands.HAND_CONNECTIONS,\n",
    "          solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "          solutions.drawing_styles.get_default_hand_connections_style())\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365f8c17-7705-4792-90a4-d0c0e4e8f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1725962900.174924 1518134 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1725962900.270371 1519631 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1725962900.356484 1519631 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Setup options\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# Get inference\n",
    "def get_annotation_from(frame):\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "    detection_result = detector.detect(image)\n",
    "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "    \n",
    "    return detection_result, annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7aa42d-8be4-4fdf-b614-59e4daaef99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1725962913.427498 1519633 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# define a video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "  \n",
    "while True:\n",
    "    # capture image\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        detection_result, annotation = get_annotation_from(cv2.flip(frame, 1))\n",
    "    \n",
    "        cv2.imshow('', annotation)  \n",
    "    else:\n",
    "        print(\"! No frame\")\n",
    "        \n",
    "    time.sleep(0.05)\n",
    "     \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# After the loop release the cap object\n",
    "cap.release()\n",
    "\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265e4364-78b8-44cc-8c01-ec6848553fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725965623.515630 1616558 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M2\n",
      "W0000 00:00:1725965623.543058 1666171 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1725965623.563794 1666171 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'thum_tip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(image_bgr, hand_landmarks, mp_hands\u001b[38;5;241m.\u001b[39mHAND_CONNECTIONS)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Classify gesture and get the corresponding letter\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m letter \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhand_landmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# If a letter is detected, display it on the screen\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m letter:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Get the wrist landmark position to display the letter near the hand\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mclassify_gesture\u001b[0;34m(hand_landmarks)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m index_mcp\u001b[38;5;241m.\u001b[39my \u001b[38;5;129;01mand\u001b[39;00m thumb_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my):  \u001b[38;5;66;03m# Index finger up, thumb down (letter \"I\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (index_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m thumb_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;129;01mand\u001b[39;00m middle_tip\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m<\u001b[39m \u001b[43mthum_tip\u001b[49m\u001b[38;5;241m.\u001b[39mx):  \u001b[38;5;66;03m# Thumb and index make \"L\" shape\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (thumb_tip\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m>\u001b[39m index_tip\u001b[38;5;241m.\u001b[39mx \u001b[38;5;129;01mand\u001b[39;00m pinky_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39mwrist\u001b[38;5;241m.\u001b[39my):  \u001b[38;5;66;03m# Circle-like gesture for \"O\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thum_tip' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Mediapipe Hand Landmarker\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def classify_gesture(hand_landmarks):\n",
    "    # Extract landmark positions for relevant points\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "\n",
    "    # Simple heuristic to classify gestures for I, L, O, V, E, U (expand based on your needs)\n",
    "    if (index_tip.y < index_mcp.y and thumb_tip.y > wrist.y):  # Index finger up, thumb down (letter \"I\")\n",
    "        return \"I\"\n",
    "    elif (index_tip.y > thumb_tip.y and middle_tip.x< thum_tip.x):  # Thumb and index make \"L\" shape\n",
    "        return \"L\"\n",
    "    elif (thumb_tip.x > index_tip.x and pinky_tip.y <wrist.y):  # Circle-like gesture for \"O\"\n",
    "        return \"O\"\n",
    "    elif (index_tip.y < wrist.y and middle_tip.y < wrist.y and ring_tip.y > wrist.y):  # \"V\" gesture\n",
    "        return \"V\"\n",
    "    elif (index_tip.y < wrist.y and middle_tip.y < wrist.y and ring_tip.y < wrist.y):  # Open hand (letter \"E\")\n",
    "        return \"E\"\n",
    "    elif (pinky_tip.y < wrist.y and index_tip.y < wrist.y):  # Thumb and pinky make \"U\" shape\n",
    "        return \"U\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Ignoring empty frame.\")\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB.\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "\n",
    "    # Process the image and find hand landmarks\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    # Convert the image back to BGR so OpenCV can display it\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect hand landmarks and handedness\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks on the image\n",
    "            mp_drawing.draw_landmarks(image_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Classify gesture and get the corresponding letter\n",
    "            letter = classify_gesture(hand_landmarks)\n",
    "\n",
    "            # If a letter is detected, display it on the screen\n",
    "            if letter:\n",
    "                # Get the wrist landmark position to display the letter near the hand\n",
    "                wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                h, w, _ = image_bgr.shape\n",
    "                cx, cy = int(wrist.x * w), int(wrist.y * h)\n",
    "\n",
    "                # Display the letter using OpenCV\n",
    "                cv2.putText(image_bgr, letter, (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Hand Gesture Recognition', image_bgr)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33045bd8-7736-41f1-900b-c5b027da235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726208948.480701   47070 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M2\n",
      "W0000 00:00:1726208948.497905   59238 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726208948.509188   59238 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "draw_color = (255, 255, 255)  # Color for drawing\n",
    "erase_color = (0, 0, 0)        # Color for erasing\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a blank canvas to draw\n",
    "canvas = np.zeros((600, 400, 3), dtype=np.uint8)\n",
    "\n",
    "# Initialize previous position variables\n",
    "prev_x, prev_y = 0, 0\n",
    "\n",
    "# Function to draw lines on canvas\n",
    "def draw_line(canvas, start, end, color, thickness=2):\n",
    "    cv2.line(canvas, start, end, color, thickness)\n",
    "\n",
    "# Function to erase drawn areas on canvas\n",
    "def erase_area(canvas, center, radius, color):\n",
    "    cv2.circle(canvas, center, radius, color, -1)\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hand landmarks\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Draw landmarks and get hand positions\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                # Get x, y coordinates of each landmark\n",
    "                h, w, c = frame.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                if id == 8:  # Index finger tip (Left hand)\n",
    "                    # Use index finger to draw\n",
    "                    if prev_x != 0 and prev_y != 0:\n",
    "                        draw_line(canvas, (prev_x, prev_y), (cx, cy), draw_color)\n",
    "                    prev_x, prev_y = cx, cy\n",
    "\n",
    "                elif id == 12:  # Index finger tip (Right hand)\n",
    "                    # Use middle finger to erase\n",
    "                    erase_area(canvas, (cx, cy), 50, erase_color)\n",
    "\n",
    "    # Display frame and canvas\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Canvas', canvas)\n",
    "\n",
    "    # Check for key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea29d6fc-4308-4c4e-82dd-1ec926296930",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Open capture with video path\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "## Open capture with video path\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "## Initialize mediapipe hand detection function\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "## Load trained model and initialize a normalizer \n",
    "model = load(\"model.joblib\")\n",
    "normalizer = Normalizer()\n",
    "\n",
    "## Define variables for output video\n",
    "h = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "size = (w, h)\n",
    "\n",
    "## Create VideoWriter instance with variables taken from input\n",
    "outputVid = cv2.VideoWriter(\"result.avi\", cv2.VideoWriter_fourcc('M','J','P','G'), 24, size, isColor = True)\n",
    "\n",
    "## Helper function to create a bounding box around each hand.\n",
    "## Takes in video frame img and hand landmarks lm\n",
    "def createBoundingBox(img, lm):\n",
    "\n",
    "\t## Initialize empty array to store all landmarks of \n",
    "\t## hand landmark lm\n",
    "\tlm_array = np.empty((0,2), int)\n",
    "\n",
    "\t## For each landmark in hand landmark, append \n",
    "\t## minimum points to array\n",
    "\tfor _, landmark in enumerate(lm.landmark):\n",
    "\t\t\n",
    "\t\twidth, height = img.shape[1], img.shape[0]\n",
    "\t\t## Calculate minimum point between landmark\n",
    "\t\t## position and size of video frame\n",
    "\t\tlm_x = min(int(landmark.x * width), width - 1)\n",
    "\t\tlm_y = min(int(landmark.y * height), height - 1)\n",
    "\n",
    "\t\t## Create a point using the minimum for landmark\n",
    "\t\tlm_point = [np.array((lm_x, lm_y))]\n",
    "\n",
    "\t\t## Append point to array\n",
    "\t\tlm_array = np.append(lm_array, lm_point, axis=0)\n",
    "\n",
    "\t## Using built-in method boundingRect, get the x,y,w,h\n",
    "\t## from the bounding box of lm_array\n",
    "\tx, y, w, h = cv2.boundingRect(lm_array)\n",
    "\n",
    "\t## Define positions for bouding box to encapsulate hand\n",
    "\tx_min = x - 20\n",
    "\n",
    "\ty_min = y - 15\n",
    "\n",
    "\tx_max = x + w + 20\n",
    "\n",
    "\ty_max = y + h + 15\n",
    "\n",
    "\treturn [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "## While capture is open\n",
    "while(capture.isOpened()):\n",
    "\n",
    "\t## Read the frame from capture\n",
    "\tread, frame = capture.read()\n",
    "\n",
    "\tframe = cv2.flip(frame,1)\n",
    "\n",
    "\t## If frame was properly read\n",
    "\tif read == True:\n",
    "\t\t\n",
    "\t\t## Convert frame to RGB for proper mediapipe detection\n",
    "\t\trgbFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\t## Process each frame to get hand landmarks\n",
    "\t\tresults = hands.process(rgbFrame)\n",
    "\n",
    "\t\t## If results exists\n",
    "\t\tif results.multi_hand_landmarks:\n",
    "\n",
    "\t\t\t## For each hand detected\n",
    "\t\t\tfor handLms in results.multi_hand_landmarks:\n",
    "\n",
    "\t\t\t\t## Call upon createBoudningBox() method to get bounding box coordinates\n",
    "\t\t\t\tboudingBox = createBoundingBox(frame, handLms)\n",
    "\n",
    "\t\t\t\t## Draw a rectangle around each processed bounding box\n",
    "\t\t\t\tcv2.rectangle(frame, (boudingBox[0], boudingBox[1]), (boudingBox[2], boudingBox[3]), (0, 255, 0), 2)\n",
    "\n",
    "\t\t\t\t## Draw the connections between landmarks for better visualization\n",
    "\t\t\t\tmp_drawing.draw_landmarks(frame, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "\t\t\t\t## Define coords as the landmark's x and y coordinates and normalize them\n",
    "\t\t\t\tcoords = handLms.landmark\n",
    "\t\t\t\tcoords = list(np.array([[landmark.x, landmark.y] for landmark in coords]).flatten())\n",
    "\t\t\t\tcoords = normalizer.transform([coords])\n",
    "\n",
    "\t\t\t\t## Predict which letter is being gestured using the trained model\n",
    "\t\t\t\tpredicted_letter = model.predict(coords)\n",
    "\n",
    "\t\t\t\t# Write above the bouding box the predicted letter\n",
    "\t\t\t\tcv2.putText(frame, str(predicted_letter[0]),(boudingBox[0], boudingBox[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\n",
    "\t\t## Write frame with detection results to VideoWriter \n",
    "\t\t## instance outputVid\n",
    "\t\toutputVid.write(frame)\n",
    "\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "capture.release()\n",
    "outputVid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b397e58b-3d4a-4df1-a9ba-e892653c8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "#Initializations: static code\n",
    "mpHands = mp.solutions.hands\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5):\n",
    "        #when the mediapipe is first started, it detects the hands. After that it tries to track the hands\n",
    "        #as detecting is more time consuming than tracking. If the tracking confidence goes down than the\n",
    "        #specified value then again it switches back to detection\n",
    "        self.hands = mpHands.Hands(max_num_hands=max_num_hands, min_detection_confidence=min_detection_confidence,\n",
    "                                   min_tracking_confidence=min_tracking_confidence)\n",
    "\n",
    "\n",
    "    def findHandLandMarks(self, image, handNumber=0, draw=False):\n",
    "        originalImage = image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # mediapipe needs RGB\n",
    "        results = self.hands.process(image)\n",
    "        landMarkList = []\n",
    "\n",
    "        if results.multi_handedness:\n",
    "            label = results.multi_handedness[handNumber].classification[0].label  # label gives if hand is left or right\n",
    "            #account for inversion in webcams\n",
    "            if label == \"Left\":\n",
    "                label = \"Right\"\n",
    "            elif label == \"Right\":\n",
    "                label = \"Left\"\n",
    "\n",
    "\n",
    "        if results.multi_hand_landmarks:  # returns None if hand is not found\n",
    "            hand = results.multi_hand_landmarks[handNumber] #results.multi_hand_landmarks returns landMarks for all the hands\n",
    "\n",
    "            for id, landMark in enumerate(hand.landmark):\n",
    "                # landMark holds x,y,z ratios of single landmark\n",
    "                imgH, imgW, imgC = originalImage.shape  # height, width, channel for image\n",
    "                xPos, yPos = int(landMark.x * imgW), int(landMark.y * imgH)\n",
    "                landMarkList.append([id, xPos, yPos, label])\n",
    "\n",
    "            if draw:\n",
    "                mpDraw.draw_landmarks(originalImage, hand, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "        return landMarkList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b79efc-009d-4200-8435-732b8dcb9ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'handDetector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhandDetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HandDetector\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'handDetector'"
     ]
    }
   ],
   "source": [
    "from handDetector import HandDetector\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "handDetector = HandDetector(min_detection_confidence=0.7)\n",
    "webcamFeed = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    status, image = webcamFeed.read()\n",
    "    handLandmarks = handDetector.findHandLandMarks(image=image, draw=True)\n",
    "    count=0\n",
    "\n",
    "    if(len(handLandmarks) != 0):\n",
    "        #we will get y coordinate of finger-tip and check if it lies above middle landmark of that finger\n",
    "        #details: https://google.github.io/mediapipe/solutions/hands\n",
    "\n",
    "        if handLandmarks[4][3] == \"Right\" and handLandmarks[4][1] > handLandmarks[3][1]:       #Right Thumb\n",
    "            count = count+1\n",
    "        elif handLandmarks[4][3] == \"Left\" and handLandmarks[4][1] < handLandmarks[3][1]:       #Left Thumb\n",
    "            count = count+1\n",
    "        if handLandmarks[8][2] < handLandmarks[6][2]:       #Index finger\n",
    "            count = count+1\n",
    "        if handLandmarks[12][2] < handLandmarks[10][2]:     #Middle finger\n",
    "            count = count+1\n",
    "        if handLandmarks[16][2] < handLandmarks[14][2]:     #Ring finger\n",
    "            count = count+1\n",
    "        if handLandmarks[20][2] < handLandmarks[18][2]:     #Little finger\n",
    "            count = count+1\n",
    "\n",
    "    cv2.putText(image, str(count), (45, 375), cv2.FONT_HERSHEY_SIMPLEX, 5, (255, 0, 0), 25)\n",
    "    cv2.imshow(\"Volume\", image)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13d5a7-de25-4fc2-a981-52a72f131277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
